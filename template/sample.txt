Enable Mirostat sampling for controlling perplexity. (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)
mirostat 0
mirostat_eta Influences how quickly the algorithm responds to feedback from the generated text. A lower learning rate will result in slower adjustments, while a higher learning rate will make the algorithm more responsive. (Default: 0.1)
float
mirostat_eta 0.1
mirostat_tau Controls the balance between coherence and diversity of the output. A lower value will result in more focused and coherent text. (Default: 5.0)
float
mirostat_tau 5.0
num_ctx Sets the size of the context window used to generate the next token. (Default: 2048)
int
num_ctx 4096
repeat_last_n Sets how far back for the model to look back to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)
int
repeat_last_n 64
repeat_penalty Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more
strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1)
float
repeat_penalty 1.1
temperature The temperature of the model. float temperature 0.7 Increasing the temperature will make
the model answer more creatively. (Default: 0.8)
seed Sets the random number seed to use for generation. Setting this to a specific number will make the model generate the same text for the same prompt. (Default: 0)
int
seed 42
stop Sets the stop sequences to use. When this pattern is encountered the LLM will stop generating text and return. Multiple stop patterns may be set by specifying multiple separate
parameters in a modelfile.
string
stop "AI assistant:"
 stop
Tail free sampling is used to reduce the impact of less probable tokens from the output. A higher value (e.g., 2.0) will reduce the impact more, while a value of 1.0 disables this setting. (default: 1)
float
tfs_z 1
num_predict Maximum number of tokens to predict when generating text. (Default: 128, -1 = infinite generation, -2 = fill context)
int
num_predict 42
top_k Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)
top_p Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)
int
top_k 40
num_batch Determines the number of tokens int 1024 processed in parallel during inference.
Affects the speed of token generation and memory usage.
Default value is often 512, but can be adjusted to optimize performance. - Reducing num_batch can help solve Out of Memory (OOM) errors for large context models.
Explanation
What's Top-p? 'Top-p' controls the diversity of AI responses: a low 'top-p' makes output more focused and predictable, while a high 'top-p' encourages variety and surprise.
Pair with temperature to fine-tune AI creativity:
higher temperatures with high 'top-p' for bold ideas, or lower temperatures with low 'top-p' for precise answers.
Using top_p=1 essentially disables the "nucleus sampling" feature, where only the most probable tokens are considered.
This is equivalent to using full softmax probability distribution to sample the next word.
How Does Temperature Affect AI Outputs? Temperature controls the randomness of word selection. Lower temperatures lead to more predictable text, while higher temperatures allow for more novel text generation.
How Does Top-p Influence Temperature Settings in AI Language Models?
Top-p and temperature are both parameters that control the randomness of AI- generated text, but they influence outcomes in subtly different ways:
Low Temperatures (0.0 - 0.5):
Effect of Top-p: A high top_p value will have minimal impact, as the model's output is already quite deterministic. A low top_p will further constrain the model, leading to very predictable outputs.
Use Cases: Ideal for tasks requiring precise, factual responses like technical explanations or legal advice. For example, explaining a scientific concept or drafting a formal business email.
Medium Temperatures (0.5 - 0.7):
Effect of Top-p: top_p starts to influence the variety of the output. A higher top_p will introduce more diversity without sacrificing coherence.
Use Cases: Suitable for creative yet controlled content, such as writing an article on a current event or generating a business report that balances creativity with professionalism.
High Temperatures (0.8 - 1.0):
Effect of Top-p: A high top_p is crucial for introducing creativity and surprise, but may result in less coherent outputs. A lower top_p can help maintain some coherence.
Use Cases: Good for brainstorming sessions, generating creative writing prompts, or coming up with out-of-the-box ideas where a mix of novelty and relevance is appreciated.
Extra-High Temperatures (1.1 - 2.0):
Effect of Top-p: The output becomes more experimental and unpredictable, and top_p 's influence can vary widely. It's a balance between randomness and diversity.
Use Cases: Best for when you're seeking highly creative or abstract ideas, such as imagining a sci-fi scenario or coming up with a plot for a fantasy story, where coherence is less of a priority compared to novelty and uniqueness.
Certainly! I'll explain each of these parameters and their significance in language model inference:
# Good Results
PARAMETER temperature 0.6
PARAMETER top_k 80
PARAMETER top_p 0.8
PARAMETER frequency_penalty 0.9
PARAMETER num_ctx 16384
PARAMETER mirostat_eta 0.5
PARAMETER num_batch 1024
PARAMETER num_keep 256
PARAMETER num_thread 8
PARAMETER repeat_last_n 64